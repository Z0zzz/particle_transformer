{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7073e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: uproot in /opt/conda/lib/python3.10/site-packages (5.3.10)\n",
      "Requirement already satisfied: awkward>=2.4.6 in /opt/conda/lib/python3.10/site-packages (from uproot) (2.6.2)\n",
      "Requirement already satisfied: cramjam>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from uproot) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from uproot) (2023.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from uproot) (1.24.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from uproot) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from uproot) (4.12.2)\n",
      "Requirement already satisfied: awkward-cpp==30 in /opt/conda/lib/python3.10/site-packages (from awkward>=2.4.6->uproot) (30)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=2.4.6->uproot) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fairseq in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.15.1)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq) (0.29.36)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.0.6)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fairseq) (2024.7.24)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fairseq) (4.65.0)\n",
      "Requirement already satisfied: bitarray in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.9.2)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.24.4)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /opt/conda/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (2.10.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (5.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq) (12.6.20)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->fairseq) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->fairseq) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: awkward in /opt/conda/lib/python3.10/site-packages (2.6.2)\n",
      "Requirement already satisfied: awkward-cpp==30 in /opt/conda/lib/python3.10/site-packages (from awkward) (30)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /opt/conda/lib/python3.10/site-packages (from awkward) (2023.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from awkward) (6.8.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from awkward) (1.24.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from awkward) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from awkward) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->awkward) (3.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fastjet in /opt/conda/lib/python3.10/site-packages (3.4.1.3)\n",
      "Requirement already satisfied: awkward>=2 in /opt/conda/lib/python3.10/site-packages (from fastjet) (2.6.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from fastjet) (1.24.4)\n",
      "Requirement already satisfied: vector in /opt/conda/lib/python3.10/site-packages (from fastjet) (1.3.1)\n",
      "Requirement already satisfied: awkward-cpp==30 in /opt/conda/lib/python3.10/site-packages (from awkward>=2->fastjet) (30)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=2->fastjet) (2023.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=2->fastjet) (6.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from awkward>=2->fastjet) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=2->fastjet) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->awkward>=2->fastjet) (3.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting weaver-core>=0.4\n",
      "  Downloading weaver_core-0.4.17-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (1.11.1)\n",
      "Requirement already satisfied: pandas>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (3.7.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (6.0)\n",
      "Collecting awkward0>=0.15.5 (from weaver-core>=0.4)\n",
      "  Downloading awkward0-0.15.5-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uproot<5.2.0,>=4.2.0 (from weaver-core>=0.4)\n",
      "  Downloading uproot-5.1.2-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.7/342.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: awkward>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (2.6.2)\n",
      "Requirement already satisfied: vector>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (1.3.1)\n",
      "Requirement already satisfied: lz4>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (4.3.2)\n",
      "Collecting xxhash>=1.4.4 (from weaver-core>=0.4)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tables>=3.6.1 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (3.7.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from weaver-core>=0.4) (2.11.2)\n",
      "Requirement already satisfied: awkward-cpp==30 in /opt/conda/lib/python3.10/site-packages (from awkward>=1.8.0->weaver-core>=0.4) (30)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=1.8.0->weaver-core>=0.4) (2023.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=1.8.0->weaver-core>=0.4) (6.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from awkward>=1.8.0->weaver-core>=0.4) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from awkward>=1.8.0->weaver-core>=0.4) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->weaver-core>=0.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.3->weaver-core>=0.4) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.3->weaver-core>=0.4) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.1->weaver-core>=0.4) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.1->weaver-core>=0.4) (3.1.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from tables>=3.6.1->weaver-core>=0.4) (2.8.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (3.4.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (2.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.2.0->weaver-core>=0.4) (0.40.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->weaver-core>=0.4) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->awkward>=1.8.0->weaver-core>=0.4) (3.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->weaver-core>=0.4) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->weaver-core>=0.4) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->weaver-core>=0.4) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->weaver-core>=0.4) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->weaver-core>=0.4) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->weaver-core>=0.4) (3.2.2)\n",
      "Installing collected packages: xxhash, awkward0, uproot, weaver-core\n",
      "  Attempting uninstall: uproot\n",
      "    Found existing installation: uproot 5.3.10\n",
      "    Uninstalling uproot-5.3.10:\n",
      "      Successfully uninstalled uproot-5.3.10\n",
      "Successfully installed awkward0-0.15.5 uproot-5.1.2 weaver-core-0.4.17 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install uproot\n",
    "!pip install fairseq\n",
    "!pip install awkward\n",
    "!pip install fastjet\n",
    "!pip install 'weaver-core>=0.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db19095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "#from weaver.nn.model.ParticleTransformer import ParticleTransformer\n",
    "#from weaver.utils.logger import _logger\n",
    "import torch.optim as optim\n",
    "#from EfficientParticleTransformer import EfficientParticleTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22674da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_and_labels(tree, transform_features=True):\n",
    "\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label_*'])\n",
    "\n",
    "    # compute new features\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "    a['part_d0'] = np.tanh(a['part_d0val'])\n",
    "    a['part_dz'] = np.tanh(a['part_dzval'])\n",
    "\n",
    "    # apply standardization\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "        a['part_d0err'] = _clip(a['part_d0err'], 0, 1)\n",
    "        a['part_dzerr'] = _clip(a['part_dzerr'], 0, 1)\n",
    "\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'], # not used in ParT\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log',\n",
    "            'part_logptrel',\n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_charge',\n",
    "            'part_isChargedHadron',\n",
    "            'part_isNeutralHadron',\n",
    "            'part_isPhoton',\n",
    "            'part_isElectron',\n",
    "            'part_isMuon',\n",
    "            'part_d0',\n",
    "            'part_d0err',\n",
    "            'part_dz',\n",
    "            'part_dzerr',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
    "    out['label'] = np.stack([a[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8efdb3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Particle Transformer (ParT)\n",
    "\n",
    "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
    "'''\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "#from fairseq import utils\n",
    "#from fairseq.incremental_decoding_utils import with_incremental_state\n",
    "#from fairseq.modules.quant_noise import quant_noise\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter\n",
    "from weaver.utils.logger import _logger\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
    "        outputs.append(lnds2)\n",
    "\n",
    "    # the following features are not symmetric for (i, j)\n",
    "    if num_outputs > 5:\n",
    "        xj_boost = boost(xj, xij)\n",
    "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        outputs.append(costheta)\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        deltarap = rapi - rapj\n",
    "        deltaphi = delta_phi(phii, phij)\n",
    "        outputs += [deltarap, deltaphi]\n",
    "\n",
    "    assert (len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def build_sparse_tensor(uu, idx, seq_len):\n",
    "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
    "    # return: (N, C, seq_len, seq_len)\n",
    "    batch_size, num_fts, num_pairs = uu.size()\n",
    "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
    "    i = torch.cat((\n",
    "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
    "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
    "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "    ), dim=0)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        i, uu.flatten(),\n",
    "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
    "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SequenceTrimmer(nn.Module):\n",
    "\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    q = min(1, random.uniform(*self.target))\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
    "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
    "            normalize_input=True, activation='gelu', eps=1e-8,\n",
    "            for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairwise_lv_dim = pairwise_lv_dim\n",
    "        self.pairwise_input_dim = pairwise_input_dim\n",
    "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
    "        self.remove_self_pair = remove_self_pair\n",
    "        self.mode = mode\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
    "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "            for dim in dims:\n",
    "                module_list.extend([\n",
    "                    nn.Conv1d(input_dim, dim, 1),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                ])\n",
    "                input_dim = dim\n",
    "            if use_pre_activation_pair:\n",
    "                module_list = module_list[:-1]\n",
    "            self.embed = nn.Sequential(*module_list)\n",
    "        elif self.mode == 'sum':\n",
    "            if pairwise_lv_dim > 0:\n",
    "                input_dim = pairwise_lv_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "            if pairwise_input_dim > 0:\n",
    "                input_dim = pairwise_input_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.fts_embed = nn.Sequential(*module_list)\n",
    "        else:\n",
    "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
    "\n",
    "    def forward(self, x, uu=None):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        # uu: (batch, v_dim, seq_len, seq_len)\n",
    "        assert (x is not None or uu is not None)\n",
    "        with torch.no_grad():\n",
    "            if x is not None:\n",
    "                batch_size, _, seq_len = x.size()\n",
    "            else:\n",
    "                batch_size, _, seq_len, _ = uu.size()\n",
    "            if self.is_symmetric and not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
    "                                          device=(x if x is not None else uu).device)\n",
    "                if x is not None:\n",
    "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    xj = x[:, :, j, i]\n",
    "                    x = self.pairwise_lv_fts(xi, xj)\n",
    "                if uu is not None:\n",
    "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    uu = uu[:, :, i, j]\n",
    "            else:\n",
    "                if x is not None:\n",
    "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
    "                    if self.remove_self_pair:\n",
    "                        i = torch.arange(0, seq_len, device=x.device)\n",
    "                        x[:, :, i, i] = 0\n",
    "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
    "                if uu is not None:\n",
    "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
    "            if self.mode == 'concat':\n",
    "                if x is None:\n",
    "                    pair_fts = uu\n",
    "                elif uu is None:\n",
    "                    pair_fts = x\n",
    "                else:\n",
    "                    pair_fts = torch.cat((x, uu), dim=1)\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
    "        elif self.mode == 'sum':\n",
    "            if x is None:\n",
    "                elements = self.fts_embed(uu)\n",
    "            elif uu is None:\n",
    "                elements = self.embed(x)\n",
    "            else:\n",
    "                elements = self.embed(x) + self.fts_embed(uu)\n",
    "\n",
    "        if self.is_symmetric and not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "        self.interaction = None\n",
    "        self.pre_mask_attn_weights = None  # To store attention weights before mask is applied\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "        )\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "    def getAttention(self):\n",
    "        return self.interaction\n",
    "    def getPreMaskAttention(self):\n",
    "        return self.pre_mask_attn_weights\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape (seq_len, batch, embed_dim)\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape (1, batch, embed_dim)\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape (batch, seq_len) where padding\n",
    "                elements are indicated by `1.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape (seq_len, batch, embed_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
    "        else:\n",
    "            residual = x\n",
    "\n",
    "\n",
    "            x = self.pre_attn_norm(x)\n",
    "\n",
    "\n",
    "            x= self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                          attn_mask=attn_mask, average_attn_weights=False)[0]  # (seq_len, batch, embed_dim)\n",
    "            y= self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                          attn_mask=attn_mask, average_attn_weights=False)[1]\n",
    "            self.interaction = y\n",
    "\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParticleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=10,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[64, 64, 64],\n",
    "                 pair_embed_dims=[32, 32, 32],\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 num_cls_layers=1,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.attention_matrix = []\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
    "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
    "        self.pairMatrixes = []\n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
    "\n",
    "        self.pair_extra_dim = pair_extra_dim\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.interactionMatrix = None\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dim\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def getAttention(self):\n",
    "        return self.attention_matrix\n",
    "\n",
    "    def getInteraction(self):\n",
    "        return self.interactionMatrix\n",
    "\n",
    "    def getPairs(self):\n",
    "        return self.pairMatrixes\n",
    "\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
    "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if uu_idx is not None:\n",
    "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
    "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "\n",
    "            # transform\n",
    "            #num = 0\n",
    "            for block in self.blocks:\n",
    "\n",
    "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "                self.interactionMatrix = attn_mask\n",
    "                #if num == 0 :\n",
    "                self.attention_matrix.append(block.interaction)\n",
    "                #num = num + 1\n",
    "\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "\n",
    "\n",
    "            return output\n",
    "\n",
    "class ParticleTransformerTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask)\n",
    "\n",
    "\n",
    "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if pf_uu_idx is not None:\n",
    "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
    "\n",
    "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
    "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask, uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119382f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981d794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31fde7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325d4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download(url, fname, chunk_size=1024):\n",
    "    '''https://gist.github.com/yanqd0/c13ed29e29432e3cf3e7c38467f42f51'''\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(fname, 'wb') as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6cd3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JetClass_example_100k.root: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 130M/130M [00:06<00:00, 21.9MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the example file\n",
    "example_file = 'JetClass_example_100k.root'\n",
    "if not os.path.exists(example_file):\n",
    "    _download('https://hqu.web.cern.ch/datasets/JetClass/example/JetClass_example_100k.root', example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd55d534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m             x[idx, :\u001b[38;5;28mlen\u001b[39m(trunc)] \u001b[38;5;241m=\u001b[39m trunc\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 25\u001b[0m tree \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39mopen(\u001b[43mexample_file\u001b[49m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m build_features_and_labels(tree)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_file' is not defined"
     ]
    }
   ],
   "source": [
    "def _clip(a, a_min, a_max):\n",
    "    try:\n",
    "        return np.clip(a, a_min, a_max)\n",
    "    except ValueError:\n",
    "        return ak.unflatten(np.clip(ak.flatten(a), a_min, a_max), ak.num(a))\n",
    "\n",
    "def _pad(a, maxlen, value=0, dtype='float32'):\n",
    "    if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "        return a\n",
    "    elif isinstance(a, ak.Array):\n",
    "        if a.ndim == 1:\n",
    "            a = ak.unflatten(a, 1)\n",
    "        a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "        return ak.values_astype(a, dtype)\n",
    "    else:\n",
    "        x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "        for idx, s in enumerate(a):\n",
    "            if not len(s):\n",
    "                continue\n",
    "            trunc = s[:maxlen].astype(dtype)\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        return x\n",
    "    \n",
    "    \n",
    "tree = uproot.open(example_file)['tree']\n",
    "data = build_features_and_labels(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f90e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ParticleTransformerWrapper(torch.nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = ParticleTransformer(**kwargs)\n",
    "        self.attention_matrix = None\n",
    "        self.interactionMatrix = None\n",
    "        self.pre_mask_attention_matrices = []\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'mod.cls_token', }\n",
    "\n",
    "    def forward(self, points, features, lorentz_vectors, mask):\n",
    "        output = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        self.attention_matrix = self.mod.getAttention()\n",
    "        self.interactionMatrix = self.mod.getInteraction()\n",
    "        #self.pre_mask_attention_matrices = self.get_pre_mask_attention_matrices()\n",
    "        return output\n",
    "\n",
    "    def get_attention_matrix(self):\n",
    "        return self.attention_matrix\n",
    "\n",
    "    def get_interactionMatrix(self):\n",
    "        return self.interactionMatrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(**kwargs):\n",
    "\n",
    "    cfg = dict(\n",
    "        input_dim=17,\n",
    "        num_classes=10,\n",
    "        # network configurations\n",
    "        pair_input_dim=4,\n",
    "        use_pre_activation_pair=False,\n",
    "        embed_dims=[128, 512, 128],\n",
    "        pair_embed_dims=[64, 64, 64],\n",
    "        num_heads=8,\n",
    "        num_layers=8,\n",
    "        num_cls_layers=2,\n",
    "        block_params=None,\n",
    "        cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "        fc_params=[],\n",
    "        activation='gelu',\n",
    "        # misc\n",
    "        trim=True,\n",
    "        for_inference=False,\n",
    "    )\n",
    "    cfg.update(**kwargs)\n",
    "\n",
    "    model = ParticleTransformerWrapper(**cfg)\n",
    "\n",
    "    model_info = {\n",
    "\n",
    "    }\n",
    "\n",
    "    return model, model_info\n",
    "\n",
    "\n",
    "def get_loss(data_config, **kwargs):\n",
    "    return torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e227f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249/903046824.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('../models/ParT_full.pt', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "state_dict = torch.load('../models/ParT_full.pt', map_location=torch.device('cpu'))\n",
    "model[0].load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd0f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
